{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ccfed9-3863-4534-8a48-aaf7cd3e8bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Original Data Head ---\n",
      "            Cipla_close   TATACON_close   BEL_close \n",
      "Date                                                \n",
      "2025-06-02        1470.2          1120.4      387.50\n",
      "2025-05-30        1465.7          1106.3      384.60\n",
      "2025-05-29        1476.9          1109.8      386.80\n",
      "2025-05-28        1468.5          1121.4      390.45\n",
      "2025-05-27        1480.5          1138.4      385.40\n",
      "\n",
      "--- Data Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 248 entries, 2025-06-02 to 2024-06-03\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Cipla_close     248 non-null    float64\n",
      " 1   TATACON_close   248 non-null    float64\n",
      " 2   BEL_close       248 non-null    float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 7.8 KB\n",
      "\n",
      "--- Data Description ---\n",
      "       Cipla_close   TATACON_close   BEL_close \n",
      "count    248.000000      248.000000  248.000000\n",
      "mean    1515.797581     1069.459677  296.545927\n",
      "std       65.277478      100.860690   26.615936\n",
      "min     1384.700000      889.450000  244.600000\n",
      "25%     1471.537500      966.637500  281.175000\n",
      "50%     1500.450000     1091.700000  292.900000\n",
      "75%     1552.987500     1155.225000  306.962500\n",
      "max     1680.500000     1256.900000  390.450000\n",
      "\n",
      "--- Daily Returns Head ---\n",
      "            Cipla_close   TATACON_close   BEL_close \n",
      "Date                                                \n",
      "2025-05-30     -0.003061       -0.012585   -0.007484\n",
      "2025-05-29      0.007641        0.003164    0.005720\n",
      "2025-05-28     -0.005688        0.010452    0.009436\n",
      "2025-05-27      0.008172        0.015160   -0.012934\n",
      "2025-05-26      0.002499        0.006940   -0.002206\n",
      "\n",
      "--- Daily Returns Info ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 247 entries, 2025-05-30 to 2024-06-03\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   Cipla_close     247 non-null    float64\n",
      " 1   TATACON_close   247 non-null    float64\n",
      " 2   BEL_close       247 non-null    float64\n",
      "dtypes: float64(3)\n",
      "memory usage: 7.7 KB\n",
      "\n",
      "Portfolio Weights (Equal): [0.33333333 0.33333333 0.33333333]\n",
      "\n",
      "--- Portfolio Returns Head ---\n",
      "Date\n",
      "2025-05-30   -0.007710\n",
      "2025-05-29    0.005508\n",
      "2025-05-28    0.004734\n",
      "2025-05-27    0.003466\n",
      "2025-05-26    0.002411\n",
      "dtype: float64\n",
      "Mean Portfolio Return: -0.0002\n",
      "Standard Deviation of Portfolio Returns: 0.0128\n",
      "\n",
      "--- Market Risk Metrics ---\n",
      "Historical VaR (99% Confidence Level): -0.0277 (or -2.77%)\n",
      "Historical CVaR / Expected Shortfall (99% Confidence Level): -0.0329 (or -3.29%)\n",
      "\n",
      "Parametric (Delta-Normal) VaR (99% Confidence Level): -0.0299 (or -2.99%)\n",
      "Parametric CVaR / Expected Shortfall (99% Confidence Level): -0.0005 (or -0.05%)\n",
      "\n",
      "Monte Carlo VaR (99% Confidence Level, 10000 simulations): -0.0303 (or -3.03%)\n",
      "Monte Carlo CVaR / Expected Shortfall (99% Confidence Level, 10000 simulations): -0.0347 (or -3.47%)\n",
      "\n",
      "--- VaR Model Backtesting (Kupiec's Proportion of Failures Test) ---\n",
      "Total Observations (T): 247\n",
      "Observed VaR Exceptions (N): 3\n",
      "Expected VaR Exceptions: 2.47\n",
      "Observed Proportion of Failures (p_hat): 0.0121\n",
      "Kupiec's LR_POF Test Statistic: 0.1075\n",
      "Critical Value (Chi-squared, 1 df, 5% significance): 3.8415\n",
      "Conclusion: DO NOT REJECT the VaR model.\n",
      "The observed number of exceptions is not statistically significantly different from the expected number.\n",
      "\n",
      "--- CVaR Model Backtesting (Average Exceedance Test) ---\n",
      "Number of VaR violations: 3\n",
      "Average Actual Loss on VaR Violation Days: -0.0329 (or -3.29%)\n",
      "Predicted Historical CVaR (99% Confidence Level): -0.0329 (or -3.29%)\n",
      "Qualitative CVaR Backtest: Model's predicted CVaR is consistent with average actual losses on violation days.\n",
      "\n",
      "--- Scenario Analysis & Stress Testing ---\n",
      "\n",
      "Current Asset Prices (as of last data point):\n",
      "Cipla_close       1452.35\n",
      "TATACON_close     1069.15\n",
      "BEL_close          318.65\n",
      "Name: 2024-06-03 00:00:00, dtype: float64\n",
      "Current Portfolio Value (based on assumed $1,000,000.00 initial investment): $1,000,000.00\n",
      "\n",
      "--- Stress Test Results ---\n",
      "\n",
      "Scenario: Market Downturn (All stocks -15%)\n",
      "  Stressed Portfolio Value: $1,000,000.00\n",
      "  Loss Under Stress: $0.00\n",
      "  Percentage Loss: 0.00%\n",
      "\n",
      "Scenario: Tech Sector Shock (BEL -25%)\n",
      "  Stressed Portfolio Value: $1,000,000.00\n",
      "  Loss Under Stress: $0.00\n",
      "  Percentage Loss: 0.00%\n",
      "\n",
      "Scenario: Pharma Sell-off (Cipla -20%)\n",
      "  Stressed Portfolio Value: $1,000,000.00\n",
      "  Loss Under Stress: $0.00\n",
      "  Percentage Loss: 0.00%\n",
      "\n",
      "Scenario: 2008 Global Financial Crisis (Approx. -35% across board)\n",
      "  Stressed Portfolio Value: $1,000,000.00\n",
      "  Loss Under Stress: $0.00\n",
      "  Percentage Loss: 0.00%\n",
      "\n",
      "Scenario: COVID-19 Market Decline (Approx. -30% across board)\n",
      "  Stressed Portfolio Value: $1,000,000.00\n",
      "  Loss Under Stress: $0.00\n",
      "  Percentage Loss: 0.00%\n",
      "\n",
      "--- FRTB (Fundamental Review of the Trading Book) Considerations ---\n",
      "FRTB is a key regulatory framework for market risk capital requirements.\n",
      "Our project's components contribute to FRTB compliance as follows:\n",
      "\n",
      "1. Internal Model Approach (IMA) vs. Standardised Approach (SA):\n",
      "   - IMA requires banks to demonstrate robust risk models (VaR, ES, Stress Testing).\n",
      "   - Our Historical, Parametric, and Monte Carlo VaR/ES calculations are foundational for IMA.\n",
      "   - SA involves prescribed risk sensitivities and risk factors, which would require a different calculation methodology.\n",
      "\n",
      "2. Expected Shortfall (ES):\n",
      "   - FRTB mandates ES (at 97.5% confidence) as the primary risk measure for IMA, replacing VaR.\n",
      "   - Our project explicitly calculates Historical, Parametric, and Monte Carlo CVaR (ES), aligning with this requirement.\n",
      "\n",
      "3. Backtesting:\n",
      "   - FRTB requires rigorous backtesting of ES models (though the specific tests are still evolving).\n",
      "   - Our Kupiec's POF test for VaR is a basic form of backtesting. For FRTB, more advanced tests (e.g., conditional coverage, magnitude tests for ES) would be necessary.\n",
      "   - Backtesting results determine whether a bank's internal model remains approved by regulators.\n",
      "\n",
      "4. Stress Testing:\n",
      "   - FRTB requires comprehensive stress testing to capture risks not fully covered by ES.\n",
      "   - Our scenario analysis and stress testing framework directly supports this, allowing for the assessment of losses under extreme market movements.\n",
      "\n",
      "--- FRTB: Non-Modellable Risk Factors (NMRF) ---\n",
      "FRTB introduces capital charges for Non-Modellable Risk Factors (NMRFs).\n",
      "These are risk factors that cannot be adequately modeled due to insufficient observable data.\n",
      "For this project, we'll conceptually illustrate how NMRF capital add-ons would be calculated.\n",
      "\n",
      "Calculating Capital Add-ons for Hypothetical NMRFs:\n",
      "  - Illiquid_Bond_Spread: Exposure=$500,000.00, Charge Rate=2.00%, Add-on=$10,000.00\n",
      "  - Exotic_FX_Volatility: Exposure=$200,000.00, Charge Rate=1.50%, Add-on=$3,000.00\n",
      "\n",
      "Total Estimated NMRF Capital Add-on: $13,000.00\n",
      "Note: This is a simplified illustration. Actual NMRF calculation involves complex methodologies and regulatory approval.\n",
      "\n",
      "--- FRTB: Default Risk Charge (DRC) ---\n",
      "FRTB introduces a separate capital charge for default risk in the trading book.\n",
      "This covers idiosyncratic and jump-to-default risks, distinct from market price risk.\n",
      "For this project, we'll conceptually illustrate a simplified DRC calculation.\n",
      "\n",
      "Calculating Simplified DRC for Hypothetical Trading Book Instruments:\n",
      "  - Corporate Bond A: EAD=$1,000,000.00, PD=1.00%, LGD=40.00%, Simplified DRC=$4,000.00\n",
      "  - Corporate Bond B: EAD=$750,000.00, PD=2.00%, LGD=35.00%, Simplified DRC=$5,250.00\n",
      "\n",
      "Total Estimated Simplified DRC: $9,250.00\n",
      "Note: This is a highly simplified illustration of DRC. Actual FRTB DRC is complex, involving specific formulas for idiosyncratic and systematic risk components, and is calibrated to a 99.9% confidence level over a 1-year horizon.\n",
      "\n",
      "--- FRTB: P&L Attribution (PLA) ---\n",
      "FRTB requires a P&L Attribution test to ensure that the risk factors used in the internal risk model adequately explain the daily P&L of the trading desk.\n",
      "This involves comparing 'Hypothetical P&L' (from risk factor changes) with 'Actual P&L'.\n",
      "This project does not currently implement PLA. It would require:\n",
      "  - Daily P&L data for the portfolio.\n",
      "  - A mapping of P&L to specific risk factors.\n",
      "  - Calculation of P&L explained by the model's risk factors vs. unexplained P&L.\n",
      "  - Statistical tests (e.g., Spearman's rank correlation, Kolmogorov-Smirnov test) to compare the distributions of hypothetical and actual P&L.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os # To check if the file exists\n",
    "from scipy.stats import norm # For Parametric VaR and CVaR\n",
    "from scipy.stats import chi2 # For Kupiec's backtesting test\n",
    "\n",
    "# --- Configuration ---\n",
    "# Define the path to your Excel file. Make sure this file is in the same directory\n",
    "# as your Python script, or provide the full path to it.\n",
    "excel_file_path = 'Historical_Portfolio.xlsx'\n",
    "sheet_name = 'portfolio' # Changed as per your update\n",
    "\n",
    "# Define the confidence level for VaR and ES calculation\n",
    "confidence_level = 0.99\n",
    "\n",
    "# For Monte Carlo Simulation\n",
    "num_simulations = 10000 # Number of simulated portfolio returns\n",
    "\n",
    "# --- Data Loading and Preparation ---\n",
    "\n",
    "# Check if the Excel file exists before attempting to load\n",
    "if not os.path.exists(excel_file_path):\n",
    "    print(f\"Error: The file '{excel_file_path}' was not found.\")\n",
    "    print(\"Please ensure the Excel file is in the correct directory.\")\n",
    "else:\n",
    "    try:\n",
    "        # Load the historical stock price data from the specified sheet\n",
    "        # We'll set the 'Date' column as the index for time-series analysis\n",
    "        df_prices = pd.read_excel(excel_file_path, sheet_name=sheet_name, index_col='Date')\n",
    "\n",
    "        print(\"--- Original Data Head ---\")\n",
    "        print(df_prices.head())\n",
    "        print(\"\\n--- Data Info ---\")\n",
    "        df_prices.info()\n",
    "        print(\"\\n--- Data Description ---\")\n",
    "        print(df_prices.describe())\n",
    "\n",
    "        # Calculate daily returns for each stock\n",
    "        df_returns = df_prices.pct_change()\n",
    "\n",
    "        # Drop the first row which will contain NaN values due to pct_change()\n",
    "        df_returns = df_returns.dropna()\n",
    "\n",
    "        print(\"\\n--- Daily Returns Head ---\")\n",
    "        print(df_returns.head())\n",
    "        print(\"\\n--- Daily Returns Info ---\")\n",
    "        df_returns.info()\n",
    "\n",
    "        # --- Portfolio Construction (Assuming Equal Weighting for now) ---\n",
    "        num_assets = df_returns.shape[1]\n",
    "        portfolio_weights = np.array([1/num_assets] * num_assets)\n",
    "        print(f\"\\nPortfolio Weights (Equal): {portfolio_weights}\")\n",
    "\n",
    "        # Calculate daily portfolio returns\n",
    "        portfolio_returns = df_returns.dot(portfolio_weights)\n",
    "\n",
    "        print(\"\\n--- Portfolio Returns Head ---\")\n",
    "        print(portfolio_returns.head())\n",
    "        \n",
    "        # Calculate mean and standard deviation of portfolio returns for Parametric and Monte Carlo VaR\n",
    "        portfolio_mean_return = portfolio_returns.mean()\n",
    "        portfolio_std_dev = portfolio_returns.std()\n",
    "\n",
    "        print(f\"Mean Portfolio Return: {portfolio_mean_return:.4f}\")\n",
    "        print(f\"Standard Deviation of Portfolio Returns: {portfolio_std_dev:.4f}\")\n",
    "\n",
    "        # --- Historical Value-at-Risk (VaR) Calculation ---\n",
    "        # Sort the portfolio returns in ascending order (from worst loss to best gain)\n",
    "        sorted_returns = portfolio_returns.sort_values(ascending=True)\n",
    "\n",
    "        # Calculate the percentile for VaR (e.g., 0.01 for 99% confidence)\n",
    "        var_percentile = 1 - confidence_level\n",
    "\n",
    "        # Calculate Historical VaR\n",
    "        historical_var = sorted_returns.quantile(var_percentile)\n",
    "\n",
    "        print(f\"\\n--- Market Risk Metrics ---\")\n",
    "        print(f\"Historical VaR ({confidence_level*100:.0f}% Confidence Level): {historical_var:.4f} (or {historical_var*100:.2f}%)\")\n",
    "\n",
    "        # --- Historical Conditional Value-at-Risk (CVaR) / Expected Shortfall (ES) Calculation ---\n",
    "        cvar_returns_historical = sorted_returns[sorted_returns <= historical_var]\n",
    "        historical_cvar = cvar_returns_historical.mean()\n",
    "        print(f\"Historical CVaR / Expected Shortfall ({confidence_level*100:.0f}% Confidence Level): {historical_cvar:.4f} (or {historical_cvar*100:.2f}%)\")\n",
    "\n",
    "        # --- Parametric (Delta-Normal) VaR Calculation ---\n",
    "        z_score = norm.ppf(var_percentile)\n",
    "        parametric_var = portfolio_mean_return + (z_score * portfolio_std_dev)\n",
    "        print(f\"\\nParametric (Delta-Normal) VaR ({confidence_level*100:.0f}% Confidence Level): {parametric_var:.4f} (or {parametric_var*100:.2f}%)\")\n",
    "\n",
    "        # --- Parametric CVaR / Expected Shortfall Calculation ---\n",
    "        pdf_at_z_score = norm.pdf(z_score)\n",
    "        parametric_cvar = portfolio_mean_return - (portfolio_std_dev * pdf_at_z_score) / (1 - var_percentile)\n",
    "        print(f\"Parametric CVaR / Expected Shortfall ({confidence_level*100:.0f}% Confidence Level): {parametric_cvar:.4f} (or {parametric_cvar*100:.2f}%)\")\n",
    "\n",
    "\n",
    "        # --- Monte Carlo VaR Calculation ---\n",
    "        simulated_returns = np.random.normal(loc=portfolio_mean_return,\n",
    "                                             scale=portfolio_std_dev,\n",
    "                                             size=num_simulations)\n",
    "        sorted_simulated_returns = np.sort(simulated_returns)\n",
    "        monte_carlo_var = np.percentile(sorted_simulated_returns, var_percentile * 100)\n",
    "        print(f\"\\nMonte Carlo VaR ({confidence_level*100:.0f}% Confidence Level, {num_simulations} simulations): {monte_carlo_var:.4f} (or {monte_carlo_var*100:.2f}%)\")\n",
    "\n",
    "        # --- Monte Carlo CVaR / Expected Shortfall Calculation ---\n",
    "        cvar_returns_mc = sorted_simulated_returns[sorted_simulated_returns <= monte_carlo_var]\n",
    "        monte_carlo_cvar = cvar_returns_mc.mean()\n",
    "        print(f\"Monte Carlo CVaR / Expected Shortfall ({confidence_level*100:.0f}% Confidence Level, {num_simulations} simulations): {monte_carlo_cvar:.4f} (or {monte_carlo_cvar*100:.2f}%)\")\n",
    "\n",
    "\n",
    "        # --- VaR Model Backtesting (Kupiec's Proportion of Failures Test) ---\n",
    "        print(\"\\n--- VaR Model Backtesting (Kupiec's Proportion of Failures Test) ---\")\n",
    "\n",
    "        # For demonstration, we'll backtest the Historical VaR against the full historical returns.\n",
    "        # In a real-world scenario, VaR would be re-estimated daily/periodically using a rolling window.\n",
    "        \n",
    "        # Number of observations (days)\n",
    "        T = len(portfolio_returns)\n",
    "        \n",
    "        # Count the number of actual losses that exceeded the Historical VaR (exceptions)\n",
    "        # Remember VaR is typically a negative number (a loss), so 'exceeded' means more negative.\n",
    "        exceptions = portfolio_returns[portfolio_returns < historical_var].count()\n",
    "        \n",
    "        # Expected number of exceptions\n",
    "        expected_exceptions = T * (1 - confidence_level)\n",
    "        \n",
    "        # Proportion of failures (actual exceptions / total observations)\n",
    "        p_hat = exceptions / T\n",
    "        \n",
    "        # Null hypothesis: The model is accurate (observed proportion of failures equals expected proportion)\n",
    "        # H0: p_hat = (1 - confidence_level)\n",
    "        # Alternative hypothesis: The model is inaccurate\n",
    "        \n",
    "        # Kupiec's POF test statistic (Likelihood Ratio Test)\n",
    "        # L_LR_POF = -2 * ln( ( (1-p)^(T-N) * p^N ) / ( (1-p_hat)^(T-N) * p_hat^N ) )\n",
    "        # where p = 1 - confidence_level, N = exceptions, T = total observations\n",
    "        \n",
    "        # Handle cases where p_hat or (1-p_hat) might be zero to avoid log(0)\n",
    "        # Also handle cases where expected_exceptions might be zero if T is very small and confidence_level is high\n",
    "        \n",
    "        # To avoid log(0) or division by zero, use small epsilon for probabilities if they are exactly 0 or 1\n",
    "        epsilon = 1e-10\n",
    "\n",
    "        p = 1 - confidence_level\n",
    "        \n",
    "        # Numerator of the likelihood ratio (under H0)\n",
    "        L_num = (p**exceptions) * ((1-p)**(T-exceptions))\n",
    "        \n",
    "        # Denominator of the likelihood ratio (under H1, using observed p_hat)\n",
    "        # Ensure p_hat is not exactly 0 or 1 for log calculation\n",
    "        p_hat_safe = np.clip(p_hat, epsilon, 1 - epsilon)\n",
    "        L_den = (p_hat_safe**exceptions) * ((1-p_hat_safe)**(T-exceptions))\n",
    "\n",
    "        # Avoid log of zero by adding a small epsilon if L_num or L_den is zero\n",
    "        LR_POF = -2 * np.log(np.clip(L_num / L_den, epsilon, 1/epsilon))\n",
    "        \n",
    "        # Critical value for chi-squared distribution with 1 degree of freedom at 95% confidence\n",
    "        alpha_significance = 0.05 # 5% significance level\n",
    "        critical_value = chi2.ppf(1 - alpha_significance, df=1) # 1 degree of freedom for POF test\n",
    "\n",
    "        print(f\"Total Observations (T): {T}\")\n",
    "        print(f\"Observed VaR Exceptions (N): {exceptions}\")\n",
    "        print(f\"Expected VaR Exceptions: {expected_exceptions:.2f}\")\n",
    "        print(f\"Observed Proportion of Failures (p_hat): {p_hat:.4f}\")\n",
    "        print(f\"Kupiec's LR_POF Test Statistic: {LR_POF:.4f}\")\n",
    "        print(f\"Critical Value (Chi-squared, 1 df, {alpha_significance*100:.0f}% significance): {critical_value:.4f}\")\n",
    "\n",
    "        if LR_POF > critical_value:\n",
    "            print(\"Conclusion: REJECT the VaR model (too many or too few exceptions).\")\n",
    "            print(\"The observed number of exceptions is statistically significantly different from the expected number.\")\n",
    "        else:\n",
    "            print(\"Conclusion: DO NOT REJECT the VaR model.\")\n",
    "            print(\"The observed number of exceptions is not statistically significantly different from the expected number.\")\n",
    "\n",
    "        # --- CVaR Model Backtesting (Average Exceedance Test) ---\n",
    "        print(\"\\n--- CVaR Model Backtesting (Average Exceedance Test) ---\")\n",
    "\n",
    "        # Get the actual losses on VaR exception days\n",
    "        # We need to filter the portfolio returns that are less than the historical VaR\n",
    "        # These are the \"exceedances\" or \"violations\"\n",
    "        var_violations = portfolio_returns[portfolio_returns < historical_var]\n",
    "\n",
    "        if len(var_violations) > 0:\n",
    "            # Calculate the average of these actual losses (exceedances)\n",
    "            average_actual_exceedance_loss = var_violations.mean()\n",
    "            \n",
    "            print(f\"Number of VaR violations: {len(var_violations)}\")\n",
    "            print(f\"Average Actual Loss on VaR Violation Days: {average_actual_exceedance_loss:.4f} (or {average_actual_exceedance_loss*100:.2f}%)\")\n",
    "            print(f\"Predicted Historical CVaR ({confidence_level*100:.0f}% Confidence Level): {historical_cvar:.4f} (or {historical_cvar*100:.2f}%)\")\n",
    "\n",
    "            # Compare the average actual exceedance loss to the predicted Historical CVaR\n",
    "            # A simple comparison: ideally, average_actual_exceedance_loss should be close to historical_cvar.\n",
    "            # If average_actual_exceedance_loss is significantly worse (more negative) than historical_cvar,\n",
    "            # the CVaR model might be underestimating tail risk.\n",
    "            # If average_actual_exceedance_loss is significantly better (less negative) than historical_cvar,\n",
    "            # the CVaR model might be overestimating tail risk.\n",
    "\n",
    "            # You can define a tolerance level for \"close\" or use more advanced statistical tests\n",
    "            # (e.g., conditional coverage tests like Christoffersen's or tests based on tick losses)\n",
    "            # For this basic implementation, we'll just show the comparison.\n",
    "            \n",
    "            # Simple qualitative assessment:\n",
    "            if average_actual_exceedance_loss < historical_cvar:\n",
    "                print(\"Qualitative CVaR Backtest: Model might be UNDERESTIMATING tail losses (Actual average loss worse than predicted CVaR).\")\n",
    "            elif average_actual_exceedance_loss > historical_cvar:\n",
    "                print(\"Qualitative CVaR Backtest: Model might be OVERESTIMATING tail losses (Actual average loss better than predicted CVaR).\")\n",
    "            else:\n",
    "                print(\"Qualitative CVaR Backtest: Model's predicted CVaR is consistent with average actual losses on violation days.\")\n",
    "        else:\n",
    "            print(\"No VaR violations observed, cannot perform CVaR backtesting (Average Exceedance Test).\")\n",
    "\n",
    "\n",
    "        # --- Scenario Analysis and Stress Testing ---\n",
    "        print(\"\\n--- Scenario Analysis & Stress Testing ---\")\n",
    "\n",
    "        # Get the last available prices as the starting point for stress scenarios\n",
    "        # Assuming the last row of df_prices contains the most recent prices\n",
    "        current_prices = df_prices.iloc[-1]\n",
    "        print(\"\\nCurrent Asset Prices (as of last data point):\")\n",
    "        print(current_prices)\n",
    "\n",
    "        # Assuming an initial investment of $1,000,000 total value for demonstration.\n",
    "        # Distribute this value equally among assets based on their last price to find 'shares_held'.\n",
    "        initial_portfolio_value = 1_000_000\n",
    "        initial_investment_per_asset = initial_portfolio_value / num_assets\n",
    "        shares_held = initial_investment_per_asset / current_prices\n",
    "        \n",
    "        current_portfolio_value = (current_prices * shares_held).sum()\n",
    "        print(f\"Current Portfolio Value (based on assumed ${initial_portfolio_value:,.2f} initial investment): ${current_portfolio_value:,.2f}\")\n",
    "\n",
    "\n",
    "        # Define Stress Scenarios as percentage shocks\n",
    "        # Added historical scenarios based on approximate Indian market declines\n",
    "        scenarios = {\n",
    "            'Market Downturn (All stocks -15%)': {\n",
    "                'Cipla_close': -0.15,\n",
    "                'TATACON_close': -0.15,\n",
    "                'BEL_close': -0.15\n",
    "            },\n",
    "            'Tech Sector Shock (BEL -25%)': { # Assuming BEL is tech-related for this example\n",
    "                'Cipla_close': -0.05,  # Moderate impact\n",
    "                'TATACON_close': -0.05, # Moderate impact\n",
    "                'BEL_close': -0.25    # Severe impact\n",
    "            },\n",
    "            'Pharma Sell-off (Cipla -20%)': { # Assuming Cipla is pharma\n",
    "                'Cipla_close': -0.20,\n",
    "                'TATACON_close': -0.05,\n",
    "                'BEL_close': -0.05\n",
    "            },\n",
    "            '2008 Global Financial Crisis (Approx. -35% across board)': {\n",
    "                'Cipla_close': -0.35,\n",
    "                'TATACON_close': -0.35,\n",
    "                'BEL_close': -0.35\n",
    "            },\n",
    "            'COVID-19 Market Decline (Approx. -30% across board)': {\n",
    "                'Cipla_close': -0.30,\n",
    "                'TATACON_close': -0.30,\n",
    "                'BEL_close': -0.30\n",
    "            }\n",
    "        }\n",
    "\n",
    "        stress_test_results = {}\n",
    "\n",
    "        for scenario_name, shocks in scenarios.items():\n",
    "            # Apply shocks to current prices\n",
    "            stressed_prices = current_prices.copy()\n",
    "            for asset, shock_percentage in shocks.items():\n",
    "                if asset in stressed_prices.index: # Ensure the asset exists in our current_prices\n",
    "                    stressed_prices[asset] *= (1 + shock_percentage)\n",
    "            \n",
    "            # Calculate portfolio value under stress\n",
    "            stressed_portfolio_value = (stressed_prices * shares_held).sum()\n",
    "            \n",
    "            # Calculate the loss under stress\n",
    "            loss_under_stress = current_portfolio_value - stressed_portfolio_value\n",
    "            \n",
    "            stress_test_results[scenario_name] = {\n",
    "                'Stressed Portfolio Value': stressed_portfolio_value,\n",
    "                'Loss Under Stress': loss_under_stress,\n",
    "                'Percentage Loss': (loss_under_stress / current_portfolio_value) * 100\n",
    "            }\n",
    "\n",
    "        print(\"\\n--- Stress Test Results ---\")\n",
    "        for scenario_name, results in stress_test_results.items():\n",
    "            print(f\"\\nScenario: {scenario_name}\")\n",
    "            print(f\"  Stressed Portfolio Value: ${results['Stressed Portfolio Value']:,.2f}\")\n",
    "            print(f\"  Loss Under Stress: ${results['Loss Under Stress']:,.2f}\")\n",
    "            print(f\"  Percentage Loss: {results['Percentage Loss']:.2f}%\")\n",
    "\n",
    "        # Interpretation of Stress Testing:\n",
    "        # Stress testing provides insight into how the portfolio would perform under\n",
    "        # specific, predefined adverse market conditions. Unlike VaR/CVaR, which are statistical\n",
    "        # measures based on historical data or simulations, stress tests use\n",
    "        # hypothetical, extreme scenarios to assess worst-case outcomes.\n",
    "        # This is crucial for capital planning and regulatory compliance (e.g., Basel, FRTB).\n",
    "\n",
    "        # --- FRTB (Fundamental Review of the Trading Book) Considerations ---\n",
    "        print(\"\\n--- FRTB (Fundamental Review of the Trading Book) Considerations ---\")\n",
    "        print(\"FRTB is a key regulatory framework for market risk capital requirements.\")\n",
    "        print(\"Our project's components contribute to FRTB compliance as follows:\")\n",
    "\n",
    "        print(\"\\n1. Internal Model Approach (IMA) vs. Standardised Approach (SA):\")\n",
    "        print(\"   - IMA requires banks to demonstrate robust risk models (VaR, ES, Stress Testing).\")\n",
    "        print(\"   - Our Historical, Parametric, and Monte Carlo VaR/ES calculations are foundational for IMA.\")\n",
    "        print(\"   - SA involves prescribed risk sensitivities and risk factors, which would require a different calculation methodology.\")\n",
    "\n",
    "        print(\"\\n2. Expected Shortfall (ES):\")\n",
    "        print(\"   - FRTB mandates ES (at 97.5% confidence) as the primary risk measure for IMA, replacing VaR.\")\n",
    "        print(\"   - Our project explicitly calculates Historical, Parametric, and Monte Carlo CVaR (ES), aligning with this requirement.\")\n",
    "\n",
    "        print(\"\\n3. Backtesting:\")\n",
    "        print(\"   - FRTB requires rigorous backtesting of ES models (though the specific tests are still evolving).\")\n",
    "        print(\"   - Our Kupiec's POF test for VaR is a basic form of backtesting. For FRTB, more advanced tests (e.g., conditional coverage, magnitude tests for ES) would be necessary.\")\n",
    "        print(\"   - Backtesting results determine whether a bank's internal model remains approved by regulators.\")\n",
    "\n",
    "        print(\"\\n4. Stress Testing:\")\n",
    "        print(\"   - FRTB requires comprehensive stress testing to capture risks not fully covered by ES.\")\n",
    "        print(\"   - Our scenario analysis and stress testing framework directly supports this, allowing for the assessment of losses under extreme market movements.\")\n",
    "\n",
    "        # --- FRTB: Non-Modellable Risk Factors (NMRF) ---\n",
    "        print(\"\\n--- FRTB: Non-Modellable Risk Factors (NMRF) ---\")\n",
    "        print(\"FRTB introduces capital charges for Non-Modellable Risk Factors (NMRFs).\")\n",
    "        print(\"These are risk factors that cannot be adequately modeled due to insufficient observable data.\")\n",
    "        print(\"For this project, we'll conceptually illustrate how NMRF capital add-ons would be calculated.\")\n",
    "        \n",
    "        # Example NMRFs and their hypothetical capital add-ons (as a percentage of exposure or fixed amount)\n",
    "        # In a real scenario, these would be derived from regulatory guidelines or internal assessments.\n",
    "        nmrf_factors = {\n",
    "            'Illiquid_Bond_Spread': 0.02,  # 2% of exposure for illiquid bond spread risk\n",
    "            'Exotic_FX_Volatility': 0.015, # 1.5% of exposure for exotic FX vol risk\n",
    "            # Add more hypothetical NMRFs relevant to your portfolio if applicable\n",
    "        }\n",
    "\n",
    "        # For demonstration, let's assume a hypothetical exposure for each NMRF\n",
    "        # In a real bank, this would be linked to specific instruments in the trading book.\n",
    "        hypothetical_exposure_nmrf = {\n",
    "            'Illiquid_Bond_Spread': 500_000,  # $500,000 exposure\n",
    "            'Exotic_FX_Volatility': 200_000,  # $200,000 exposure\n",
    "        }\n",
    "\n",
    "        total_nmrf_capital_add_on = 0\n",
    "        print(\"\\nCalculating Capital Add-ons for Hypothetical NMRFs:\")\n",
    "        for factor, charge_rate in nmrf_factors.items():\n",
    "            exposure = hypothetical_exposure_nmrf.get(factor, 0)\n",
    "            capital_add_on = exposure * charge_rate\n",
    "            total_nmrf_capital_add_on += capital_add_on\n",
    "            print(f\"  - {factor}: Exposure=${exposure:,.2f}, Charge Rate={charge_rate*100:.2f}%, Add-on=${capital_add_on:,.2f}\")\n",
    "\n",
    "        print(f\"\\nTotal Estimated NMRF Capital Add-on: ${total_nmrf_capital_add_on:,.2f}\")\n",
    "        print(\"Note: This is a simplified illustration. Actual NMRF calculation involves complex methodologies and regulatory approval.\")\n",
    "\n",
    "        # --- FRTB: Default Risk Charge (DRC) ---\n",
    "        print(\"\\n--- FRTB: Default Risk Charge (DRC) ---\")\n",
    "        print(\"FRTB introduces a separate capital charge for default risk in the trading book.\")\n",
    "        print(\"This covers idiosyncratic and jump-to-default risks, distinct from market price risk.\")\n",
    "        print(\"For this project, we'll conceptually illustrate a simplified DRC calculation.\")\n",
    "\n",
    "        # Hypothetical trading book instruments with credit risk\n",
    "        # In a real scenario, these would be actual bonds, loans, or derivatives with counterparty risk.\n",
    "        trading_book_instruments = [\n",
    "            {'name': 'Corporate Bond A', 'EAD': 1_000_000, 'PD': 0.01, 'LGD': 0.40, 'correlation_factor': 0.5},\n",
    "            {'name': 'Corporate Bond B', 'EAD': 750_000,  'PD': 0.02, 'LGD': 0.35, 'correlation_factor': 0.5},\n",
    "            # Add more instruments as needed\n",
    "        ]\n",
    "\n",
    "        total_drc = 0\n",
    "        print(\"\\nCalculating Simplified DRC for Hypothetical Trading Book Instruments:\")\n",
    "        for instrument in trading_book_instruments:\n",
    "            # Simplified DRC calculation (ignoring netting, hedging, jump-to-default correlation for simplicity)\n",
    "            # This is a highly simplified Expected Loss component for demonstration.\n",
    "            # Actual DRC is much more involved, often using a Merton-like model or credit spread approach.\n",
    "            idiosyncratic_drc = instrument['EAD'] * instrument['PD'] * instrument['LGD']\n",
    "            \n",
    "            # For systemic risk (correlation factor), a more complex formula would be used.\n",
    "            # Here, we just add a conceptual component based on a correlation factor.\n",
    "            # This is NOT the full FRTB DRC formula but a conceptual illustration.\n",
    "            systemic_drc_component = instrument['EAD'] * instrument['PD'] * instrument['LGD'] * instrument['correlation_factor'] # Highly simplified\n",
    "            \n",
    "            # For demonstration, let's just use the idiosyncratic component as a proxy for 'risk'\n",
    "            # The actual FRTB DRC is a more complex measure of unexpected loss from default.\n",
    "            instrument_drc = idiosyncratic_drc # Using only idiosyncratic part for simplicity\n",
    "            total_drc += instrument_drc\n",
    "            print(f\"  - {instrument['name']}: EAD=${instrument['EAD']:,.2f}, PD={instrument['PD']*100:.2f}%, LGD={instrument['LGD']*100:.2f}%, Simplified DRC=${instrument_drc:,.2f}\")\n",
    "\n",
    "        print(f\"\\nTotal Estimated Simplified DRC: ${total_drc:,.2f}\")\n",
    "        print(\"Note: This is a highly simplified illustration of DRC. Actual FRTB DRC is complex, involving specific formulas for idiosyncratic and systematic risk components, and is calibrated to a 99.9% confidence level over a 1-year horizon.\")\n",
    "\n",
    "\n",
    "        # --- P&L Attribution (PLA) ---\n",
    "        print(\"\\n--- FRTB: P&L Attribution (PLA) ---\")\n",
    "        print(\"FRTB requires a P&L Attribution test to ensure that the risk factors used in the internal risk model adequately explain the daily P&L of the trading desk.\")\n",
    "        print(\"This involves comparing 'Hypothetical P&L' (from risk factor changes) with 'Actual P&L'.\")\n",
    "        print(\"This project does not currently implement PLA. It would require:\")\n",
    "        print(\"  - Daily P&L data for the portfolio.\")\n",
    "        print(\"  - A mapping of P&L to specific risk factors.\")\n",
    "        print(\"  - Calculation of P&L explained by the model's risk factors vs. unexplained P&L.\")\n",
    "        print(\"  - Statistical tests (e.g., Spearman's rank correlation, Kolmogorov-Smirnov test) to compare the distributions of hypothetical and actual P&L.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during data processing: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
